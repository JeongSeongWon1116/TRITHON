version: "3.9"

x-common-env: &common-env
  # 필요시 Hugging Face 토큰 (공개 모델이지만 rate limit 완화에 도움)
  HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN:-}
  VLLM_LOGGING_LEVEL: INFO
  VLLM_NO_USAGE_STATS: "1"
  HF_HOME: /data/hf-cache
  TRANSFORMERS_CACHE: /data/hf-cache
  HUGGINGFACE_HUB_CACHE: /data/hf-cache

x-common-deploy: &common-deploy
  restart: unless-stopped

x-common-volumes: &common-volumes
  - ./data/hf-cache:/data/hf-cache  # 모델/토큰 캐시 공유 (호스트 경로)

# ===== 모드 A: 단일 인스턴스가 3GPU 모두 사용 (tensor parallel = 3) =====
services:
  ax_vllm_tp3:
    build: .
    container_name: ax-vllm-tp3
    environment:
      <<: *common-env
    command: >
      vllm serve skt/A.X-4.0-Light
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 3
      --dtype bfloat16
      --max-model-len 16384
      --enforce-eager
      # (옵션) 도구 호출 활성화: 모델 카드 권장
      --enable-auto-tool-choice
      --tool-call-parser hermes
    volumes: *common-volumes
    ports:
      - "8000:8000"
    deploy: *common-deploy
    healthcheck:
      test: [ "CMD", "/healthcheck.sh" ]
      interval: 30s
      timeout: 5s
      retries: 5
    runtime: nvidia
    # NVIDIA Container Toolkit 사용
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      NVIDIA_VISIBLE_DEVICES: "0,1,2"

  # ===== 모드 B: GPU별 독립 인스턴스 3개 =====
  ax_vllm_gpu0:
    build: .
    container_name: ax-vllm-gpu0
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "0"
    command: >
      vllm serve skt/A.X-4.0-Light
      --host 0.0.0.0
      --port 8001
      --tensor-parallel-size 1
      --dtype bfloat16
      --max-model-len 16384
      --enforce-eager
      --enable-auto-tool-choice
      --tool-call-parser hermes
    volumes: *common-volumes
    ports:
      - "8001:8001"
    deploy: *common-deploy
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  ax_vllm_gpu1:
    build: .
    container_name: ax-vllm-gpu1
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "1"
    command: >
      vllm serve skt/A.X-4.0-Light
      --host 0.0.0.0
      --port 8002
      --tensor-parallel-size 1
      --dtype bfloat16
      --max-model-len 16384
      --enforce-eager
      --enable-auto-tool-choice
      --tool-call-parser hermes
    volumes: *common-volumes
    ports:
      - "8002:8002"
    deploy: *common-deploy
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  ax_vllm_gpu2:
    build: .
    container_name: ax-vllm-gpu2
    environment:
      <<: *common-env
      NVIDIA_VISIBLE_DEVICES: "2"
    command: >
      vllm serve skt/A.X-4.0-Light
      --host 0.0.0.0
      --port 8003
      --tensor-parallel-size 1
      --dtype bfloat16
      --max-model-len 16384
      --enforce-eager
      --enable-auto-tool-choice
      --tool-call-parser hermes
    volumes: *common-volumes
    ports:
      - "8003:8003"
    deploy: *common-deploy
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
